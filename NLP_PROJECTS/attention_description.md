# BERT Masked Language Model Project

## Overview

 This project focuses on leveraging the transformer-based language model BERT to predict masked words in a sequence of text. BERT was developed by Google and trained using a Masked Language Model approach, where it predicts masked words based on the context of surrounding words.

## How it Works

The project involves two main parts:

1. **Masked Word Prediction**:
   - Utilizes the transformers Python library, developed by Hugging Face, to implement a program for masked word prediction.
   - BERT's transformer architecture, consisting of 12 layers with 12 self-attention heads each, is employed for this task.

2. **Attention Scores Visualization**:
   - The program generates diagrams illustrating attention scores for each of the 144 self-attention heads in the BERT model.
   - Analysis of these diagrams helps in understanding what BERT's attention heads might be focusing on during language comprehension.

